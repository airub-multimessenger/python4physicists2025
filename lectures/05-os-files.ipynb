{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operating system and files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with files is important if we are dealing with more data than we want to type into to our code/have a user input. It is also important when the output of the program should be saved and easily shared.\n",
    "\n",
    "### Before we start... `import`\n",
    "To interact with the operating system and read files, we need to import the module `os`:\n",
    "```python\n",
    "import os\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we are running a unix-like OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print(platform.system())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths\n",
    "Paths identify locations (including of files) on a filesystem. \n",
    "\n",
    "Examples:\n",
    "- Windows path: `C:\\User\\Documents\\file.ext`\n",
    "- Linux path: `/home/user/file`\n",
    "- Mac path: `/Users/User/Documents/file.ext`\n",
    "\n",
    "\n",
    "In Windows, most filenames have extensions and extensions are how the OS determines the file type.\n",
    "In Linux, file type and extension are unrelated at a fundamental level but extensions are of help to the user and applications.\n",
    "\n",
    "White it is technically possible to manipulate paths as strings, **don't do it**. It's messy, ugly and does not play well across different operating systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python paths, the old way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find out our current directory\n",
    "base_dir = os.getcwd()\n",
    "print(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a path to a new directory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir = os.path.join(base_dir, 'directory_practice')\n",
    "print(new_dir)\n",
    "type(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our path, let's create the directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we try to create a directory with an existing name, it won't do anything\n",
    "if not os.path.exists(new_dir):\n",
    "    os.makedirs(new_dir)\n",
    "\n",
    "print(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python paths using `pathlib`\n",
    "Here we are still manipulating a path as a string. Can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(base_dir)\n",
    "print(base_dir)\n",
    "type(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a new directory with the `mkdir()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir = base_dir / \"directory_practice\"\n",
    "\n",
    "Path.mkdir(new_dir, exist_ok=True) \n",
    "print(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can manipulate paths as objects.\n",
    "- Better functionality in the form of class and instance methods of `Path`. We will see some examples in a moment.\n",
    "- Useful `/` operator, which lets you append subdirectory names or filenames onto a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the name of the home directory\n",
    "print(Path.home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the name of the current working directory\n",
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_dir.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_dir = base_dir / \"more_directory_practice\"\n",
    "print(second_dir.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Path.mkdir()` allows you to make a new directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.mkdir(second_dir, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = second_dir / 'file1.dat'\n",
    "file_path.touch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_path.is_file())\n",
    "print(file_path.is_dir())\n",
    "print(second_dir.is_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path.suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths can be absolute or relative. Relative paths allow you to refer succintly to directories and files within the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_path = Path(\"/Users/elisa/Desktop/teaching/PythonFall2025\")\n",
    "abs_path.is_absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = Path(\"PythonFall2025\")\n",
    "rel_path.is_absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can iterate over directory contents with `Path.iterdir()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in second_dir.iterdir():\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that everything in the directory is printed out - files and directories, but not the files in sub-directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in `Path.glob()` method allows you to search a directory for files that meet a condition. It uses unix wildcard patterns, and can operate recursively (searching sub-directories within a directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * means no character matching required - anything that ends with .dat will be printed\n",
    "for path in second_dir.glob(\"*.dat\"):\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? means one character match is required \n",
    "for path in second_dir.glob(\"file?.dat\"):\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? means one character match is required \n",
    "for path in second_dir.glob(\"file??.dat\"):\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [] means a match within a range is required \n",
    "for path in second_dir.glob(\"*[1-6].dat\"):\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search in sub-directories with **\n",
    "for path in second_dir.glob(\"**/*.dat\"):\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search in sub-directories with rglob\n",
    "for path in second_dir.rglob(\"*.dat\"):\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search recursively with glob module\n",
    "import glob\n",
    "\n",
    "files = glob.glob('**/*.dat', recursive = True)\n",
    "\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to move or delete a file or directory, you can use `replace()` and `unlink()` respectively. But be careful! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries: what is a file?\n",
    "A file generally has a\n",
    "- Header\n",
    "- Data\n",
    "- End of file (EOF)\n",
    "\n",
    "Types of files\n",
    "- Text files\n",
    "- Binary files\n",
    "\n",
    "We will start with text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First file: a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, some data\n",
    "names = [\"NGC 5128\", \"TXS 0506+056\", \"NGC 1068\", \"GB6 J1040+0617\", \"TXS 2226-184\"]\n",
    "distances = [3.7, 1.75e3, 14.4, 1.51e4, 107.1]  # Mpc\n",
    "luminosities = [1e40, 3e46, 4.9e38, 6.2e45, 5.5e41] # erg/s\n",
    "\n",
    "dataset = { 'names' : names, 'distances' : distances, 'luminosities' : luminosities }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening, writing and reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'galaxy_names.dat'\n",
    "filepath = new_dir / filename\n",
    "\n",
    "with open(filepath, 'w') as f:\n",
    "    for string in names:\n",
    "        f.write(string + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'galaxy_distances.dat'\n",
    "filepath = new_dir / filename\n",
    "\n",
    "with open(filepath, 'w') as f:\n",
    "    f.writelines(str(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `write` writes a **string**. You can also use `writelines`, which writes a **sequence**.\n",
    "\n",
    "You should close files after you are done with them. This is done automatically with the `with` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(data)\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file is really big, this is not ideal because all the file content gets loaded in a variable (on the RAM). Better to read line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "\n",
    "# You can also used f.readline() to read one line at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the file is opened in text mode for reading (option/mode `r`), meaning that:\n",
    "- only characters/string can be written;\n",
    "- everything is read as a character.\n",
    "- the file is only read, contents cannot be overwritten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options:\n",
    "- `w` - write\n",
    "- `rb` - read in binary mode\n",
    "- `rw` - write in binary mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other reading and writing options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible read and write files simultaneously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_r = new_dir / 'galaxy_names_reversed.dat'\n",
    "with open(filepath, 'r') as reader, open(filepath_r, 'w') as writer:\n",
    "    galaxy_names = reader.readlines()\n",
    "    writer.writelines(reversed(galaxy_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'a') as a_writer:\n",
    "    a_writer.write('M87 \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line endings\n",
    "There are unfortunately different ways of signaling the end of a line.\n",
    "- `\\r\\n` used by Windows\n",
    "- `\\n` used by Unix and Mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Character encodings\n",
    "**Encoding** refers to translating byte data to characters. Bytes are integers with value between 0 and 255. Data is stored in terms of bytes, then read in sequence when file is accessed, according to the encoding format. Both ASCII and Unicode are common encoding formats. ASCII is a (small) subset of Unicode, and can only store 128 characters (compared to >1 million). What can happen if you try to parse a Unicode-formatted file as ASCII?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization and deserialization\n",
    "**Serialization** means to take an object and transform it into a stream of bytes, for storage or transmission. **Deserialization** means to re-encode the object based on the stream of bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary files\n",
    "- Writing binary content by hand is complicated and messy.\n",
    "- In `python` we can use `pickle` to dump an arbitrary object into a file.\n",
    "\n",
    "When is `pickle` useful?\n",
    "\n",
    "`pickle` has four methods: \n",
    "- `dump(obj, file, protocol=None, *, fix_imports=True, buffer_callback=None)`\n",
    "- `dumps(obj, protocol=None, *, fix_imports=True, buffer_callback=None)`\n",
    "- `load(file, *, fix_imports=True, encoding=\"ASCII\", errors=\"strict\", buffers=None)`\n",
    "- `loads(bytes_object, *, fix_imports=True, encoding=\"ASCII\", errors=\"strict\", buffers=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'galaxy_binary.dat'\n",
    "filepath = new_dir / filename\n",
    "\n",
    "with open(filepath, 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works with basically any object (even your own classes), but it also very opaque:\n",
    "- `python` specific, no cross-language standard;\n",
    "- basically you need to know in advance what's inside the file;\n",
    "- writing and reading iteratively is possible but complicated;\n",
    "\n",
    "The behavior is strongly dependent on protocol version (new `pickle` versions added with new `python` versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using JSON\n",
    "- JSON (JavaScript Object Notation) is a standard encoding format that allows to write multiple data types in the form of a text file.\n",
    "- You can think of a JSON file as a big nested dictionary.\n",
    "- Most `python` native data types can be translated to JSON objects.\n",
    "    - `dict` -> `object`\n",
    "    - `list, tuple` -> `array`\n",
    "    - `str` -> `string`\n",
    "    - `int, long, float` -> `number`\n",
    "    - `True` -> `true`\n",
    "    - `False` -> `false`\n",
    "    - `None` -> `null`\n",
    " \n",
    "Serialization methods are `dump()`, which writes the data to a file in JSON format, and `dumps()`, which returns a string of the data in JSON format.\n",
    "\n",
    "Deserialization methods are `load()`, which loads a file in JSON format, and `loads()`, which loads a string of the data in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = 'galaxy_json.dat'\n",
    "filepath = new_dir / filename\n",
    "\n",
    "with open(filepath, 'w') as f:\n",
    "    json_data = json.dumps(dataset) # dumps() returns a string\n",
    "    json.dump(dataset, f) # dump() writes to file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_data)\n",
    "type(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems like python syntax, but this is JSON.\n",
    "- The file is human-readable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    obj = json.load(f)\n",
    "\n",
    "print(obj)\n",
    "type(obj) # Original type is restored!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    json_data1 = json.dumps(dataset)\n",
    "    json_data2 = json.dumps(dataset, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_data1)\n",
    "print(json_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lost in translation\n",
    "One has to be a bit careful due to the inexact mapping between python types and JSON objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tuple = (1,2,3)\n",
    "encoded_tuple = json.dumps(simple_tuple)\n",
    "decoded_tuple = json.loads(encoded_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tuple == decoded_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(simple_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(decoded_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tuple == tuple(decoded_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tuple = tuple(decoded_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tuple == new_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom types\n",
    "\n",
    "You can write objects from your own classes to JSON, however, they need to be broken down into JSON objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV\n",
    "CSV is acronym for \"comma separated values\", it is the format of choice for tabular data. A CSV files consists of lines (entries) where different values (fields) are separated, usually by commas. Content is text in ASCII or Unicode format.\n",
    "\n",
    "Fields can also be separated by tab (\\t), colon (:) and semi-colon (;) characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV files \n",
    "This can be done with the `reader` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'galaxy_csvformat.dat'\n",
    "filepath = new_dir / filename\n",
    "\n",
    "with open(filepath, 'r') as f:\n",
    "    csv_reader = csv.reader(f, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            print(f'\\t{row[0]} is located at distance {row[1]} Mpc, and has luminosity {row[2]} erg/s.')\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing CSV files\n",
    "This can be done with the `writer` object and `writerow` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'galaxy_file.csv'\n",
    "filepath = new_dir / filename\n",
    "\n",
    "with open(filepath, mode='w') as f:\n",
    "    galaxy_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    for name, dist, lum in zip(names, distances, luminosities):\n",
    "        galaxy_writer.writerow([name, dist, lum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading/writing csv files to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'galaxy_dict_file.csv'\n",
    "filepath = new_dir / filename\n",
    "\n",
    "with open(filepath, 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"name\",\"distance\", \"luminosity\"])\n",
    "    for name, dist, lum in zip(names, distances, luminosities):\n",
    "        writer.writerow({\"name\": name, \"distance\": dist, \"luminosity\": lum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    reader = csv.DictReader(f, fieldnames=[\"name\",\"distance\", \"luminosity\"])\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing csv file with a header\n",
    "filename = 'galaxy_dict_file_with_header.csv'\n",
    "filepath = new_dir / filename\n",
    "\n",
    "with open(filepath, 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"name\",\"distance\", \"luminosity\"])\n",
    "    writer.writeheader()\n",
    "    for name, dist, lum in zip(names, distances, luminosities):\n",
    "        writer.writerow({\"name\": name, \"distance\": dist, \"luminosity\": lum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    print(reader.fieldnames)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Note\n",
    "\n",
    "We will cover working with ***large*** datasets in the last lecture of the course, on working with `pandas`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a27840a7b1c12ed5c4097e0c82ce74f146693c21815a51f33b43828b75394333"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
